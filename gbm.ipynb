{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical data from cache...\n",
      "Loading historical data from cache...\n",
      "Loading historical data from cache...\n",
      "train size: 175\n",
      "validate size: 37\n",
      "test size: 39\n",
      "r-hat: 0.0035381385085537344, sigma-hat: 0.027451606159246463\n",
      "mu: 0.8919876994959023, sigma: 0.43578073791993077\n",
      "Annualized Return: 0.8916109041555411\n",
      "Annualized Volatility: 0.4357807379199307\n",
      "First close: 41.26\n",
      "Last close: 100.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [mu, sigma]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 1 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_normal_halfnormal MSE: 0.0006262979240132546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [mu, sigma]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 1 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_studentt_halfcauchy_5 MSE: 0.0006925564856443559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [mu, sigma]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 1 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_studentt_halfcauchy_10 MSE: 0.0006898228231714164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [mu, sigma]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 1 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_studentt_halfcauchy_30 MSE: 0.000677927546010468\n",
      "Best Model: model_normal_halfnormal with MSE: 0.0006262979240132546\n"
     ]
    }
   ],
   "source": [
    "from backtester import Backtester\n",
    "\n",
    "import pymc as pm\n",
    "import numpy as np \n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import itertools\n",
    "from math import sqrt\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "stock = 'UAL'\n",
    "\n",
    "\n",
    "backtester = Backtester(stock, 'SMART', 'USD')\n",
    "\n",
    "df = backtester.one_yr_1d_data\n",
    "close = df['close']\n",
    "data = np.log(df['close']).diff().dropna()\n",
    "\n",
    "# 70% train, 15% validate, 15% test\n",
    "# data is time series data, so we should split it by time\n",
    "\n",
    "train_size = int(len(data) * 0.7)\n",
    "validate_size = int(len(data) * 0.15)\n",
    "\n",
    "train_data = data[:train_size]\n",
    "validate_data = data[train_size:train_size + validate_size]\n",
    "test_data = data[train_size + validate_size:]\n",
    "\n",
    "# print size\n",
    "print('train size:', len(train_data))\n",
    "print('validate size:', len(validate_data))\n",
    "print('test size:', len(test_data))\n",
    "\n",
    "\n",
    "hyperparameters = {'dt': 1/252}\n",
    "\n",
    "\n",
    "# fit the model with train data\n",
    "r_hat = np.mean(data)\n",
    "sigma_hat = np.std(data)\n",
    "# annuallized\n",
    "dt = hyperparameters['dt']\n",
    "mu = r_hat/dt + 0.5 * sigma_hat**2\n",
    "sigma = sqrt(sigma_hat**2/dt)\n",
    "# print all these numbers\n",
    "print(f\"r-hat: {r_hat}, sigma-hat: {sigma_hat}\")\n",
    "print(f\"mu: {mu}, sigma: {sigma}\")\n",
    "\n",
    "\n",
    "# annualized returns and volatility\n",
    "annualized_return = r_hat * 252\n",
    "annualized_volatility = sigma_hat * sqrt(252)\n",
    "\n",
    "print(f\"Annualized Return: {annualized_return}\")\n",
    "print(f\"Annualized Volatility: {annualized_volatility}\")\n",
    "print(f\"First close: {close.iloc[0]}\")\n",
    "print(f\"Last close: {close.iloc[-1]}\")\n",
    "\n",
    "mu_prior = lambda: pm.Normal('mu', mu=mu, sigma=sigma * 0.1)\n",
    "sigma_scale = np.sqrt(np.pi / 2) * sigma\n",
    "sigma_prior = lambda: pm.HalfNormal('sigma', sigma=sigma_scale)\n",
    "likelihood = lambda mu, sigma: pm.Normal('returns', mu=(mu - 0.5 * sigma**2)*dt, sigma=sigma * np.sqrt(dt), observed=train_data)\n",
    "\n",
    "prior_configs = {\n",
    "    \"model_normal_halfnormal\": {\n",
    "        \"mu_prior\": mu_prior,\n",
    "        \"sigma_prior\": sigma_prior,\n",
    "        \"likelihood\": likelihood\n",
    "    },\n",
    "    \"model_studentt_halfcauchy_5\": {\n",
    "        \"mu_prior\": lambda: pm.StudentT('mu', nu=5, mu=0, sigma=1),\n",
    "        \"sigma_prior\": lambda: pm.HalfCauchy('sigma', beta=0.5),\n",
    "        \"likelihood\": lambda mu, sigma: pm.StudentT('returns', nu=5, mu=mu - 0.5 * sigma**2, sigma=sigma, observed=train_data)\n",
    "    },\n",
    "    \"model_studentt_halfcauchy_10\": {\n",
    "        \"mu_prior\": lambda: pm.StudentT('mu', nu=10, mu=0, sigma=1),\n",
    "        \"sigma_prior\": lambda: pm.HalfCauchy('sigma', beta=0.5),\n",
    "        \"likelihood\": lambda mu, sigma: pm.StudentT('returns', nu=10, mu=mu - 0.5 * sigma**2, sigma=sigma, observed=train_data)\n",
    "    },\n",
    "    \"model_studentt_halfcauchy_30\": {\n",
    "        \"mu_prior\": lambda: pm.StudentT('mu', nu=30, mu=0, sigma=1),\n",
    "        \"sigma_prior\": lambda: pm.HalfCauchy('sigma', beta=0.5),\n",
    "        \"likelihood\": lambda mu, sigma: pm.StudentT('returns', nu=30, mu=mu - 0.5 * sigma**2, sigma=sigma, observed=train_data)\n",
    "    }\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name, config in prior_configs.items():\n",
    "    with pm.Model() as model:\n",
    "        # Define priors\n",
    "        mu = config[\"mu_prior\"]()\n",
    "        sigma = config[\"sigma_prior\"]()\n",
    "\n",
    "        # Define likelihood\n",
    "        likelihood = config[\"likelihood\"](mu, sigma)\n",
    "\n",
    "        # Sample from posterior\n",
    "        trace = pm.sample(\n",
    "            draws=1000,\n",
    "            tune=500,\n",
    "            target_accept=0.8,\n",
    "            chains=4,\n",
    "            cores=4,\n",
    "            random_seed=42\n",
    "        )\n",
    "\n",
    "        # Posterior predictive checks on validation data\n",
    "        # We'll generate posterior predictive samples conditioned on the fitted parameters.\n",
    "        # To do this properly, we need to define a predictive model.\n",
    "        # Since the validation data is out-of-sample, we just simulate what the model would predict.\n",
    "        # For simplicity, we assume the same model form applies: returns ~ distribution(mu - 0.5*sigma^2, sigma)\n",
    "        # We'll generate predictions for the validation time indices, even though it doesn't condition on new covariates.\n",
    "        \n",
    "        # One approach:\n",
    "        # We can't directly \"observe\" validation_data in the same model run. Instead, we can:\n",
    "        # 1. Extract posterior samples of mu and sigma from 'trace'.\n",
    "        # 2. For each sample, simulate synthetic validation returns from the same likelihood distribution.\n",
    "        \n",
    "        post_mu_samples = trace.posterior['mu'].values.flatten()\n",
    "        post_sigma_samples = trace.posterior['sigma'].values.flatten()\n",
    "        \n",
    "        # Generate posterior predictive samples for validation data:\n",
    "        # We'll just draw from the predictive distribution:\n",
    "        # returns ~ LikelihoodDistribution(mu - 0.5 * sigma^2, sigma)\n",
    "        n_post_samples = len(post_mu_samples)\n",
    "        n_val = len(validate_data)\n",
    "        \n",
    "        # For demonstration, we assume the same process applies to validation:\n",
    "        # If you're modeling time-series with drift over time, you'd need a time-series forecasting approach.\n",
    "        predictive_samples = np.empty((n_post_samples, n_val))\n",
    "        for i in range(n_post_samples):\n",
    "            mu_pred = post_mu_samples[i]\n",
    "            sigma_pred = post_sigma_samples[i]\n",
    "            # Draw from the specified distribution:\n",
    "            # If Normal: np.random.normal(mu_pred - 0.5 * sigma_pred**2, sigma_pred, size=n_val)\n",
    "            # If StudentT: add logic for StudentT as well.\n",
    "            # We'll assume Normal for demonstration:\n",
    "            # Adjust depending on which model you're checking.\n",
    "            if \"StudentT\" in model_name:\n",
    "                # for StudentT:\n",
    "                nu = 5  # hard-coded since we used it in priors\n",
    "                predictive_samples[i, :] = pm.draws_from_random_variable(\n",
    "                    pm.distributions.continuous.StudentT.dist(nu=nu, mu=mu_pred - 0.5 * sigma_pred**2, sigma=sigma_pred), \n",
    "                    draws=n_val,\n",
    "                    random_seed=(42 + i)\n",
    "                )\n",
    "            else:\n",
    "                # Normal:\n",
    "                predictive_samples[i, :] = np.random.normal(mu_pred - 0.5 * sigma_pred**2, sigma_pred, size=n_val)\n",
    "\n",
    "        # Compute an evaluation metric on validation_data\n",
    "        # For example, Mean Squared Error:\n",
    "        mean_predictions = predictive_samples.mean(axis=0)\n",
    "        mse = np.mean((validate_data - mean_predictions)**2)\n",
    "\n",
    "        model_results[model_name] = {\n",
    "            \"trace\": trace,\n",
    "            \"mse\": mse\n",
    "        }\n",
    "        print(f\"{model_name} MSE: {mse}\")\n",
    "\n",
    "# After looping through all models, select the model with the best performance:\n",
    "best_model = min(model_results, key=lambda k: model_results[k][\"mse\"])\n",
    "print(f\"Best Model: {best_model} with MSE: {model_results[best_model]['mse']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibkr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
